{
 "cells": [
  {
   "cell_type": "raw",
   "id": "430aef69-811d-4ad5-a375-3fa5da76897b",
   "metadata": {},
   "source": [
    "Part 1: Understanding Regularization\n",
    "\n",
    "Q1.what is regularization in the context of deep learning? Why is it important?\n",
    "\n",
    "ANS:\n",
    "Regularization is a technique used in machine learning, including deep learning, to prevent overfitting by adding a penalty term to the loss function. It is important because overfitting occurs when a model learns the training data too well, capturing noise and irrelevant patterns, which reduces its ability to generalize to unseen data.\n",
    "\n",
    "Q2Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff.\n",
    "\n",
    "ANS:\n",
    "The bias-variance tradeoff refers to the balance between a model's ability to capture the underlying patterns in the data (low bias) and its tendency to fit noise (high variance). Regularization helps address this tradeoff by penalizing complex models, reducing their variance at the cost of introducing some bias. This prevents the model from becoming too complex and overfitting the training data.\n",
    "\n",
    "Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model?\n",
    "ANS:\n",
    "L1 regularization (Lasso): adds the absolute value of the magnitude of coefficients as a penalty term to the loss function. It encourages sparsity in the model by driving some coefficients to zero.\n",
    "\n",
    "L2 regularization (Ridge) adds the squared magnitude of coefficients as a penalty term to the loss function. It penalizes large weights but does not force them to zero as aggressively as L1 regularization.\n",
    "In summary, L1 regularization can lead to sparse models with some coefficients being exactly zero, while L2 regularization tends to produce models with small but non-zero coefficients.\n",
    "\n",
    "Q3 Discuss the role of regularization in preventing overfitting and improving the generalization of deep learning models.\n",
    "Regularization helps prevent overfitting by discouraging overly complex models that fit the training data too closely. By adding a penalty term to the loss function, regularization encourages the model to learn simpler patterns that generalize better to unseen data, thus improving its generalization performance.\n",
    "\n",
    "Part 2: Regularization Techniques\n",
    "\n",
    "Q4.Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inference.\n",
    "\n",
    "ANS:\n",
    "Dropout is a regularization technique where randomly selected neurons are ignored during training. This forces the network to learn redundant representations of the data, making it more robust and less likely to overfit. During inference, the outputs of all neurons are scaled by the dropout probability, effectively averaging the predictions of many different thinned networks, which reduces overfitting.\n",
    "\n",
    "Q5.Describe the concept of Early Stopping as a form of regularization. How does it help prevent overfitting during the training process?\n",
    "ANS:\n",
    "\n",
    "Early stopping involves monitoring the model's performance on a validation set during training and stopping the training process when the performance begins to degrade. It prevents overfitting by stopping the training before the model starts to fit noise in the training data too closely, thus improving its ability to generalize.\n",
    "\n",
    "Q6:Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfitting?\n",
    "ANS:\n",
    "\n",
    "Batch Normalization is a technique that normalizes the inputs of each layer to have zero mean and unit variance. It helps prevent overfitting by reducing internal covariate shift, which stabilizes the training process and allows for higher learning rates. Additionally, it acts as a form of regularization by introducing noise to the network during training, similar to Dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e491256-bc3c-467d-b018-675f170bda3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 06:34:24.951768: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 06:34:24.956668: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 06:34:25.032041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-12 06:34:26.362796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8138 - loss: 0.5827\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9463 - loss: 0.1843\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9577 - loss: 0.1409\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9652 - loss: 0.1164\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9674 - loss: 0.1069\n",
      "313/313 - 1s - 2ms/step - accuracy: 0.9770 - loss: 0.0749\n",
      "\n",
      "Test accuracy: 0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Define the model architecture with Dropout regularization\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout layer with 50% dropout probability\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout layer with 50% dropout probability\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c24d71-7028-4ab2-9903-1704cbde6510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
